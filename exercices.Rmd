---
title: "Notes"
author: "Joan Claverol Romero"
date: "30/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F)
```

## Machine learning

```{r}
library(tidyverse)
library(magrittr)
```

Load data:

```{r}
dataset <- read_csv("datasets/Part 2 - Regression/Section 5 - Multiple Linear Regression/50_Startups.csv")
# add dummies
library(fastDummies)
dataset %<>% 
  fastDummies::dummy_cols("State", remove_first_dummy = T)
# Mahine learning process
library(caret)
set.seed(123)
train_id <- createDataPartition(y = dataset$Profit, p = 0.8, list = F)
train <- dataset[train_id,]
test <- dataset[train_id,]
```

Time to create the model:

```{r}
mod <- lm(Profit ~., data = train %>% select(-State))
summary(mod)
```

Podemos contruir un modelo para que seleccione las variables:

* Eliminación hacia atrás automàtica:

```{r}
# especificamos un p valor
backwardElimination <- function(x, sl) {
  numVars = length(x)
  for (i in c(1:numVars)){
    regressor = lm(formula = Profit ~ ., data = x)
    maxVar = max(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"])
    if (maxVar > sl){
      j = which(coef(summary(regressor))[c(2:numVars), "Pr(>|t|)"] == maxVar)
      x = x[, -j]
    }
    numVars = numVars - 1
  }
  return(summary(regressor))
}
```

Applying the function:

```{r}
SL = 0.05
dataset = dataset[, c(1,2,3,4,5)]
backwardElimination(dataset, SL)

```


## Regresión polinómica

Cargamos los datos:

```{r}
dataset <- read_csv("datasets/Part 2 - Regression/Section 6 - Polynomial Regression/Position_Salaries.csv")
ggplot(dataset, aes(x = Level, y = Salary)) +
  geom_point() +
  geom_smooth(se = F)
```

Partimos los datos:

```{r}
# Mahine learning process
library(caret)
set.seed(123)
train_id <- createDataPartition(y = dataset$Salary, p = 0.8, list = F)
train <- dataset[train_id,]
test <- dataset[train_id,]
```

Creamos el primer modelo:

```{r}
mod <- lm(Salary ~ Level, dataset)
summary(mod)
```

Visualizemos los resultados:

```{r}
library(modelr)
dataset %>% 
  add_predictions(model = mod) %>% 
  ggplot(aes(x = Level)) +
    geom_point(aes(y = Salary)) +
    geom_line(aes(y = pred), color = "red")
```


Vamos a ajustar con un model de regresion polinomica:

```{r}
# anadimos nuevas variables
df <- dataset %>% 
  mutate(
    level2 = Level^2,
    level3 = Level^3,
    level4 = Level^4,
    # level5 = Level^5
    )
mod_poly <- lm(Salary ~ ., df %>% select(-Position))
summary(mod_poly)
```

```{r}
df %>% 
  add_predictions(model = mod_poly) %>% 
  ggplot(aes(x = Level)) +
    geom_point(aes(y = Salary)) +
    geom_line(aes(y = pred), color = "red")
```

## Support Vector Machine

```{r}
dataset <- read_csv("datasets/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)/Position_Salaries.csv")
```

Anadimos nuevas variables

```{r}
mod_svm <- e1071::svm(Salary ~ ., 
                      dataset %>% select(-Position), 
                      type = "eps-regression", 
                      kernel = "radial")
summary(mod_svm)
```

```{r}
df %>% 
  add_predictions(model = mod_svm) %>% 
  ggplot(aes(x = Level)) +
    geom_point(aes(y = Salary)) +
    geom_line(aes(y = pred), color = "red")
```

## Arboles de regresion

Arboles de classificacion y de regression. 

Nos centraremos en los arboles de regresion. 
El algoritmo mira la entropia, como de juntos o de dispersos pueden estar esos puntos. Intenta agrupar en cierto punto una serie de puntos. Buscamos explicar al maximo de puntos con cada particion. 
Como funciona?

```{r}
dataset <- read_csv("datasets/Part 2 - Regression/Section 8 - Decision Tree Regression/Position_Salaries.csv")
```

Vamos a usar el paquete rpart:


```{r}
mod_rpart <- rpart::rpart(Salary ~ ., 
                          dataset %>% select(-Position),
                          control = rpart::rpart.control(
                            minsplit = 1 # numero de splits dentro del arbol
                            )
                          )
summary(mod_rpart)
```

```{r}
df %>% 
  add_predictions(model = mod_rpart) %>% 
  ggplot(aes(x = Level)) +
    geom_point(aes(y = Salary)) +
    geom_line(aes(y = pred), color = "red")
```

## Bosques aleatorios

Es un algoritmo de aprendizaje conjunto. 

1. Primero elgiriremos un conjunto aleatorio K de puntos de datos del conjunto de entrenamiento. 
2. Contruir un arbol de decision asociado a esos K puntos de datos. 
3. Elegir el ntree de arboles que queremos construir y repetimos los pasos 1 y 2. 
4. Para un nuevo punto de datos, hacer que cada uno de los ntree arboles hafa una prediccion del valor de Y para el punto en cuestion, y asigne al nuevo punto la prediccion final basada en el promedio de ttodas las predicciones Y de los arboles. 

```{r}
dataset <- read_csv("datasets/Part 2 - Regression/Section 8 - Decision Tree Regression/Position_Salaries.csv")
```

Vamos a usar el paquete rpart:


```{r}
mod_rf <- randomForest::randomForest(Salary ~ ., 
                          dataset %>% select(-Position), 
                          ntree = 100
                          )
mod_rf
```

```{r}
df %>% 
  add_predictions(model = mod_rf) %>% 
  ggplot(aes(x = Level)) +
    geom_point(aes(y = Salary)) +
    geom_line(aes(y = pred), color = "red")
```

# Clasificacion

## Regresion logistica

```{r}
dataset <- read_csv("datasets/Part 3 - Classification/Section 14 - Logistic Regression/Social_Network_Ads.csv")
library(magrittr)
dataset %<>%
  select(Purchased, EstimatedSalary, Age)
```

Hacemos la particion:

```{r}
set.seed(123)
train_id <- createDataPartition(y = dataset$Purchased, p = 0.75, list = F)
train <- dataset[train_id,]
test <- dataset[train_id,]
```

Creamos el modelo

```{r}
classifier <- glm(Purchased ~., 
                  data = train,
                  family = binomial)
summary(classifier)
```

Now we can do the predicions

```{r}
data_pred <- test %>% 
  add_predictions(model = classifier, type = "response") %>% 
  mutate(purchased_pred = if_else(pred > 0.5, 1, 0),
         resid = abs(Purchased - purchased_pred))
```

And check the confusion matrix

```{r}
confusionMatrix(data = factor(data_pred$purchased_pred), 
                reference = factor(data_pred$Purchased))
```

Let's do the visualizations of the training set

```{r}
data_pred %>% 
  ggplot(aes(x = Age, y = EstimatedSalary)) +
    geom_point(aes(fill = pred, col = factor(resid)), 
               shape = 21, size = 3, stroke = 1) +
    scale_color_manual(values = c("white", "red")) + 
    labs(color = "Errors", fill = "Prob") +
    theme_minimal()
```

## KNN

1. Primero elegimos el numero de vecinos para la categoria, normalmente usamos un valor impar.
2. Luego tomamos los K vecinos a traves de la distancia euclidea (puede ser la distancia Manhattan, o otras). 
3. Contamos el numero de puntos que pertenecen a cada categoria
4. Asiganmos el nuevo dato a la categoria con mas vecinos en ella

Creamos el modelo con los anteriores datos:

```{r}
classifier <- caret::train(factor(Purchased) ~.,
                           data = train, 
                           method = "kknn",
                           preProcess = c("center","scale"))
classifier
```

Now we can do the predicions

```{r}
data_pred <- test %>% 
  add_predictions(model = classifier, type = "raw") %>% 
  mutate(resid = abs(Purchased - if_else(as.integer(pred) == 1, 0, 1)))
```

And check the confusion matrix

```{r}
confusionMatrix(data = factor(data_pred$pred), 
                reference = factor(data_pred$Purchased))
```

Let's do the visualizations of the training set

```{r}
data_pred %>% 
  ggplot(aes(x = Age, y = EstimatedSalary)) +
    geom_point(aes(fill = pred, col = factor(resid)), 
               shape = 21, size = 3, alpha = 0.5) +
      geom_point(aes(col = factor(resid)), shape = 21, size = 3, stroke = 1) +
    scale_color_manual(values = c("white", "red")) + 
    labs(color = "Errors", fill = "Class") +
    theme_minimal()
```

## SVM

Support Vector Machine pasa por el concepto del margen maximo. La distancia maxima que tiene ara equivocarse. Buscan los puntos de clases distintas que sean equidistantes. Estod dos puntos sob los que llamamos los punots de soporte. 

```{r}
# caret 
classifier <- caret::train(factor(Purchased) ~.,
                           data = train, 
                           method = "svmRadial",
                           preProcess = c("center","scale"))

classifier
```

Now we can do the predicions

```{r}
data_pred <- test %>% 
  add_predictions(model = classifier, type = "raw") %>% 
  mutate(resid = abs(Purchased - if_else(as.integer(pred) == 1, 0, 1)))
```

And check the confusion matrix

```{r}
confusionMatrix(data = factor(data_pred$pred), 
                reference = factor(data_pred$Purchased))
```

Let's do the visualizations of the training set

```{r}
data_pred %>% 
  ggplot(aes(x = Age, y = EstimatedSalary)) +
    geom_point(aes(fill = pred, col = factor(resid)), 
               shape = 21, size = 3, alpha = 0.5) +
      geom_point(aes(col = factor(resid)), shape = 21, size = 3, stroke = 1) +
    scale_color_manual(values = c("white", "red")) + 
    labs(color = "Errors", fill = "Class") +
    theme_minimal()
```

## Naive bayes

usamos la libreria naive bayes y predecimos solo con el estimated salay

```{r}
# e1071
classifier <- e1071::naiveBayes(factor(Purchased) ~.,
                                data = train)
# caret 
# classifier <- caret::train(factor(Purchased) ~.,
#                            data = train, 
#                            method = "nb",
#                            preProcess = c("scale"))

classifier
```

```{r}
data_pred <- test %>% 
  add_predictions(model = classifier, type = "class") %>% 
  mutate(resid = abs(Purchased - if_else(as.integer(pred) == 1, 0, 1)))
```

And check the confusion matrix

```{r}
confusionMatrix(data = factor(data_pred$pred), 
                reference = factor(data_pred$Purchased))
```

Let's do the visualizations of the training set

```{r}
data_pred %>% 
  ggplot(aes(x = Age, y = EstimatedSalary)) +
    geom_point(aes(fill = pred), shape = 21, size = 3, alpha = 0.5) +
    geom_point(aes(col = factor(resid)), shape = 21, size = 3, stroke = 1) +
    scale_color_manual(values = c("white", "red")) + 
    labs(color = "Errors", fill = "Class") +
    theme_minimal()
```

## Arboles de decision

CART (classification and regression tree)

```{r}
# e1071
classifier <- rpart::rpart(factor(Purchased) ~.,
                                data = train)
# caret 
classifier <- caret::train(factor(Purchased) ~.,
                           data = train,
                           method = "rpart",
                           preProcess = c("center"))

classifier
```

```{r}
data_pred <- test %>% 
  add_predictions(model = classifier, type = "raw") %>% 
  mutate(resid = abs(Purchased - if_else(as.integer(pred) == 1, 0, 1)))
```

And check the confusion matrix

```{r}
confusionMatrix(data = factor(data_pred$pred), 
                reference = factor(data_pred$Purchased))
```

Let's do the visualizations of the training set

```{r}
data_pred %>% 
  ggplot(aes(x = Age, y = EstimatedSalary)) +
    geom_point(aes(fill = pred), shape = 21, size = 3, alpha = 0.5) +
    geom_point(aes(col = factor(resid)), shape = 21, size = 3, stroke = 1) +
    scale_color_manual(values = c("white", "red")) + 
    labs(color = "Errors", fill = "Class") +
    theme_minimal()
```

## Random forest 

1. Se seleccionar un numero aleatorio K de puntyos del conjunto de entrenamiento. 
2. Construir el arbol de decision asociado a esos puntos de datos
3. Elegir el numero de Ntree de arboles que queremos construir y repetir los pasos 1 y 2
4. Para classificar un nuevo punto, hacer que cada uno de los ntree arboles elabore la preduccion de a que categoria pertenece y asignar el nuevo punto a la categoria con mas votos. 

```{r}
# e1071
classifier <- randomForest::randomForest(factor(Purchased) ~.,
                                         data = train)
# caret 
classifier <- caret::train(factor(Purchased) ~.,
                           data = train,
                           method = "rf",
                           preProcess = c("center"))

classifier
```

```{r}
data_pred <- test %>% 
  add_predictions(model = classifier, type = "raw") %>% 
  mutate(resid = abs(Purchased - if_else(as.integer(pred) == 1, 0, 1)))
```

And check the confusion matrix

```{r}
confusionMatrix(data = factor(data_pred$pred), 
                reference = factor(data_pred$Purchased))
```

Let's do the visualizations of the training set

```{r}
data_pred %>% 
  ggplot(aes(x = Age, y = EstimatedSalary)) +
    geom_point(aes(fill = pred), shape = 21, size = 3, alpha = 0.5) +
    geom_point(aes(col = factor(resid)), shape = 21, size = 3, stroke = 1) +
    scale_color_manual(values = c("white", "red")) + 
    labs(color = "Errors", fill = "Class") +
    theme_minimal()
```

## Metricas de evaulacion de modelos de clasificacion

### Las curvas cap

Estas curvas de perfil de precison acumulada o cap, nos permiten entender como evaluar nuestro modelo. 

CAP = perfil de precision acumulado
ROC = caractersiticas operativa del receptor

# Clustering

## K-Means

En este caso tendremos un conjunto de puntos que queremos agrupar sin tener la variable por la que empezaremos. 

Como funciona:

1. elegir el numero K de clusters
2. Seleccionar al azar K puntos, los baricentros (no necesariamente de nuestro dataset)
3. Asignar cada punto al baricentro mas cercano -> esto formara los K clusters
4. Calcular y asignar el nuevo baricentro de cada cluster
5. Reasignar cada punto de los datos a su baricentro mas cercano. Si ha habido nuevas asignaciones, ir al PASO 4, si no ir FIN. 

!Importante al elegir los baricentros iniciales. Usando el algoritmo de K-Means++ evitamos este problema. 

Como seleccionar el numero de clusters; suma de los quedrados de los centros de los grupos. 


```{r}
dataset <- read_csv("datasets/Part 4 - Clustering/Section 24 - K-Means Clustering/Mall_Customers.csv")
dataset <- dataset %>% 
  select(Anual_Income = `Annual Income (k$)`, Spending_Score = `Spending Score (1-100)`)
```

Empezamos detectando el numero adecuado de k-means:

```{r}
set.seed(6)
wcss <- vector()
for (i in 1:10) {
  wcss[i] <- sum(kmeans(x = dataset, centers = i)$withinss)
}
plot(x = 1:10, y = wcss, type = "b", main = "Metodo del codo", 
     xlab = "Number of clusters (k)", ylab = "WCSS (k)")
```

AHora ya tenemos la informacion sabiendo que K debe ser 5:

```{r}
set.seed(29)
# Aplicando el alogritmo con k - optimo
kmeans <- kmeans(x = dataset, centers = 5, iter.max = 300, nstart = 10)
kmeans
```

Vamos a visualizar los clusters:

```{r}
library(cluster)
cluster::clusplot(x = dataset, clus = kmeans$cluster, lines = 0, shade = T, 
                  color = T, label = 4,  plotchar = F, span = T, 
                  main = "Clustering clients", xlab = "Ingresos anuales", 
                  ylab = "Puntuacion (1-100)")
```

Como anadir los datos en el dataset:

```{r}
dataset$kmeans_clsuters <- kmeans$cluster
dataset
```

## Clustering jerarquico

Diferencia entre un clustering aglomerativo o divisitivo. 

Para un paso aglomerativo: 

1. Hace r que casa punto se un propio cluster -> asi tendremos N clusters
2. Elegir lod dos puntos mas cercanos y juntarlos en un unico clister (n-1 clusters)
3. Elego los dos **dos clusters mas cercanos** y juntarlos en un unico cluster (N - 2 clusters)
4. Repetir el PASO 3 hasta solo tener 1 cluster

Como funcionan los clustering jerarquicos

Podemos usar la distancia vertical mas alta dentro de un cluster. 
Localizar la distancia mas larga de las lineas verticales. Y cortad por la mitad. Pero no puede cortar ninguna linea horizontal. 


```{r}
dataset <- read_csv("datasets/Part 4 - Clustering/Section 25 - Hierarchical Clustering/Mall_Customers.csv")
dataset <- dataset %>% 
  select(Anual_Income = `Annual Income (k$)`, Spending_Score = `Spending Score (1-100)`)
```

Vamos a crear el dendograma 

```{r}
dendogram <- hclust(dist(x = dataset, method = "euclidean"), method = "ward.D")
plot(dendogram)
```

Cantidad optima de clusters es 5, igual que el que obtuvimos en kmeans. 

```{r}
# dendograma optimizado
hc <- hclust(dist(x = dataset, method = "euclidean"), method = "ward.D")
# vamos a cortar el dendograma (un arbol)
y_hc <- cutree(tree = hc, k = 5)
clusplot(x = dataset, clus = y_hc, lines = 0, shade = T, 
                  color = T, label = 4,  plotchar = F, span = T, 
                  main = "Clustering clients", xlab = "Ingresos anuales", 
                  ylab = "Puntuacion (1-100)")
```

Nos han salido el mismo cluster que con K-means. 

